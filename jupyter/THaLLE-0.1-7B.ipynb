{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c779799e-f773-4d15-aa93-329a82a011bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ngrok config add-authtoken 2gGtpjM3ivyQ8lfhWJOsyPDvqj2_6KpKBT71RvvFbv8zQVcgD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dfd1ab5-96d2-4165-ab21-19cbda79e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ngrok http http://localhost:8501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2efbd79-9fd9-40b4-9a1a-25819da9d50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 15 09:49:29 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n",
      "|  0%   51C    P2             73W /  450W |   24073MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f92a6-9894-469e-85ed-2e509e601956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178bc188b180491095ca8f15991a81e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54dbbcd6cb248b1833bb63a5682c783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00037.safetensors:   0%|          | 10.5M/3.76G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.huggingface.co/repos/0d/ee/0deecfb2b45a159252701d3bf1cb56f185d30cc6e44e2f5899d977f0c84fe140/60ea416808b446ee22e0912107f316e17075848bb5baa4630cf78244ae6f1b07?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00001-of-00037.safetensors%3B+filename%3D%22model-00001-of-00037.safetensors%22%3B&Expires=1718680823&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxODY4MDgyM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzBkL2VlLzBkZWVjZmIyYjQ1YTE1OTI1MjcwMWQzYmYxY2I1NmYxODVkMzBjYzZlNDRlMmY1ODk5ZDk3N2YwYzg0ZmUxNDAvNjBlYTQxNjgwOGI0NDZlZTIyZTA5MTIxMDdmMzE2ZTE3MDc1ODQ4YmI1YmFhNDYzMGNmNzgyNDRhZTZmMWIwNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=H58SrjQGUkdy9BspopdcSPYtthtuMp6xvp6YQfOv-kqv-DMMNI2zOBH6vRR5ZqimOHL3PgTZo-7FvABDmFyYGxNGUoUm7P2Qu4T%7Ek-UJIH2YkrUjQgdD1M-vP0uU7%7EzD8tLkpbA4jAnSHPn4ISBng3H3swmmS8265zu1PFPlslsJ0INgt8Q3z5w9AZoNUi%7EzebmvFCe1J2ql3MITWCSCqNgQ3KKxTqCeQe%7Es%7EHZpoV9Vc5T3yKMNc43V62QwwNoYc63iYAcA4cSeIc5zQBSGJBaNKHkBcx8Bp1DkCN0kUp%7E12KyWOFpZbPJuoEYxGD1RM6%7E8xVpZkWlPrOekcgqfaA__&Key-Pair-Id=K2FPYV99P2N66Q: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1dbd905dc634632bead12e67621978f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00037.safetensors:   1%|          | 21.0M/3.76G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.huggingface.co/repos/0d/ee/0deecfb2b45a159252701d3bf1cb56f185d30cc6e44e2f5899d977f0c84fe140/60ea416808b446ee22e0912107f316e17075848bb5baa4630cf78244ae6f1b07?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00001-of-00037.safetensors%3B+filename%3D%22model-00001-of-00037.safetensors%22%3B&Expires=1718680823&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxODY4MDgyM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzBkL2VlLzBkZWVjZmIyYjQ1YTE1OTI1MjcwMWQzYmYxY2I1NmYxODVkMzBjYzZlNDRlMmY1ODk5ZDk3N2YwYzg0ZmUxNDAvNjBlYTQxNjgwOGI0NDZlZTIyZTA5MTIxMDdmMzE2ZTE3MDc1ODQ4YmI1YmFhNDYzMGNmNzgyNDRhZTZmMWIwNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=H58SrjQGUkdy9BspopdcSPYtthtuMp6xvp6YQfOv-kqv-DMMNI2zOBH6vRR5ZqimOHL3PgTZo-7FvABDmFyYGxNGUoUm7P2Qu4T%7Ek-UJIH2YkrUjQgdD1M-vP0uU7%7EzD8tLkpbA4jAnSHPn4ISBng3H3swmmS8265zu1PFPlslsJ0INgt8Q3z5w9AZoNUi%7EzebmvFCe1J2ql3MITWCSCqNgQ3KKxTqCeQe%7Es%7EHZpoV9Vc5T3yKMNc43V62QwwNoYc63iYAcA4cSeIc5zQBSGJBaNKHkBcx8Bp1DkCN0kUp%7E12KyWOFpZbPJuoEYxGD1RM6%7E8xVpZkWlPrOekcgqfaA__&Key-Pair-Id=K2FPYV99P2N66Q: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e93f25137914f1ea5e9059b2f6812d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00037.safetensors:   2%|1         | 62.9M/3.76G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from flask import Flask, request, jsonify\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# ล้างหน่วยความจำ GPU ก่อนโหลดโมเดล\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# แจ้งการโหลดโมเดลและ tokenizer\n",
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-72B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-72B\")\n",
    "print(\"Model and tokenizer loaded.\")\n",
    "\n",
    "# ย้ายโมเดลไปยัง GPU ถ้ามี\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "    print(\"Model moved to GPU.\")\n",
    "else:\n",
    "    print(\"GPU not available, using CPU.\")\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate():\n",
    "    torch.cuda.empty_cache()  # ล้างหน่วยความจำก่อนการประมวลผลใหม่\n",
    "    text = request.form['input_text']\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # ย้าย tensors ไปยัง GPU ถ้ามี\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(inputs, max_length=100, num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    torch.cuda.empty_cache()  # ล้างหน่วยความจำหลังจากการประมวลผล\n",
    "    return jsonify({'generated_text': generated_text})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=8501)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6b695dd-58bd-422b-b08e-08c2654c4899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Traceback (most recent call last):\n",
      "  File \"/notebooks/app.py\", line 7, in <module>\n",
      "    model = model.to(\"cuda\")\n",
      "NameError: name 'model' is not defined\n"
     ]
    }
   ],
   "source": [
    "!python3 app.py --server.port 8501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833dca8b-6fb5-4b3a-944a-22e1532d5d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb86cea-c9f4-439f-8673-bef687c0c3f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746e7ad3-5608-4ee9-966a-224dda5230f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8909307a-5c33-4959-8685-9e3db22424bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask\n",
      "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n",
      "Collecting itsdangerous>=2.1.2 (from flask)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from flask) (1.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->flask) (2.1.5)\n",
      "Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: itsdangerous, flask\n",
      "Successfully installed flask-3.0.3 itsdangerous-2.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install flask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03b84b4-1695-4854-abc5-56a11590207e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a31a9978d124846a8fe886ff2eaf474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:8501\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [14/Jun/2024 23:58:37] \"POST /generate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [15/Jun/2024 00:02:32] \"POST /generate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [15/Jun/2024 00:05:00] \"POST /generate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [15/Jun/2024 00:06:26] \"POST /generate HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, render_template\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# โหลดโมเดลและ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KBTG-Labs/THaLLE-0.1-7B-fa\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"KBTG-Labs/THaLLE-0.1-7B-fa\")\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate():\n",
    "    text = request.form['input_text']\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs, max_length=200, num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return jsonify({'generated_text': generated_text})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=8501)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
